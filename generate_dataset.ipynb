{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f87585-8e7f-45d6-90c0-fe2700e06579",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import os\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c9934d-d239-4b11-b68d-ec3013b235f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_CATEGORIES = {\n",
    "    'clothing': ['shirt', 'pants', 'dress', 'jeans', 'coat', 'blazer', 'bikini', 'sweater', 'cardigen',\n",
    "                 'jacket', 'skirt', 'vest', 'suit', 'blouse', 'pajama', 'underwear'],\n",
    "    'animal': ['dog', 'cat', 'bird', 'fish', 'lion', 'tiger', 'wolf', \n",
    "               'fox', 'deer', 'cow', 'pig', 'sheep', 'goat'],\n",
    "    'color': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'pink', \n",
    "              'purple', 'brown', ],\n",
    "    'body part': ['hand', 'leg', 'ear', 'eye', 'nose', 'mouth', 'arm', 'foot']\n",
    "}\n",
    "\n",
    "# OOD_CATEGORIES = {\n",
    "#     'fruit': ['apple', 'banana', 'cherry', 'grape', 'lemon', 'lime', 'berry',\n",
    "#               'peach', 'pear', 'plum', 'melon', 'mango', 'kiwi', 'fig', 'coconut',\n",
    "#              'guava', 'prune', 'papaya', ],\n",
    "#     'food': ['bread', 'cheese', 'meat', 'rice', 'pasta', 'soup', 'salad', \n",
    "#              'cake', 'cookie', 'candy', 'egg'],\n",
    "#     'job': ['doctor', 'nurse', 'teacher', 'lawyer', 'chef', 'artist', 'writer', \n",
    "#             'driver', 'pilot', 'farmer', 'builder', 'singer', 'dancer', 'actor', 'judge'],\n",
    "#     'building': [\"house\", \"school\", \"hospital\", \"hotel\", \"office\", \"factory\", \n",
    "#                  \"church\", \"temple\", \"stadium\", \"museum\", \"library\", \"airport\", \n",
    "#                  \"barn\", \"cabin\", \"skyscraper\", \"cottage\", \"castle\"]\n",
    "# }\n",
    "\n",
    "# Distractor words (common nouns that don't fit target categories)\n",
    "DISTRACTORS = [\n",
    "    'book', 'paper', 'stone', 'metal', 'plastic', 'rubber',\n",
    "    'string', 'wire', 'box', 'bag', 'cup', 'plate', 'pot', 'pan',\n",
    "    'clock', 'phone', 'computer', 'screen', 'keyboard', 'camera',\n",
    "    'music', 'sound', 'voice', 'word', 'letter', 'number', 'symbol', 'sign',\n",
    "    'door', 'window', 'wall', 'floor', 'roof', 'stairs', 'path', 'road',\n",
    "    'water', 'fire', 'air', 'wind', 'rain', 'ice',\n",
    "    'space', 'time', 'day', 'night', 'morning'\n",
    "]\n",
    "len(DISTRACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162a65df-0c9d-41c9-9b2f-47f71ff7e643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping word: jeans -> [3841, 598]\n",
      "Skipping word: blazer -> [2067, 23697]\n",
      "Skipping word: bikini -> [65, 1609, 6729]\n",
      "Skipping word: sweater -> [82, 906, 977]\n",
      "Skipping word: cardigen -> [5057, 6569]\n",
      "Skipping word: jacket -> [73, 5827]\n",
      "Skipping word: skirt -> [4991, 2154]\n",
      "Skipping word: blouse -> [2067, 1559]\n",
      "Skipping word: pajama -> [79, 1662, 3105]\n",
      "Skipping word: underwear -> [8154, 23581]\n",
      "Skipping word: tiger -> [83, 7420]\n",
      "Skipping word: sheep -> [32158, 752]\n",
      "Skipping word: goat -> [3427, 266]\n",
      "Skipping word: nose -> [77, 974]\n",
      "Skipping word: plastic -> [501, 5174]\n",
      "Skipping word: rubber -> [60530, 655]\n",
      "Skipping word: morning -> [57902, 1251]\n",
      "Skipping word: lion -> [75, 295]\n",
      "Skipping word: deer -> [2934, 263]\n",
      "Skipping word: pig -> [79, 328]\n",
      "Skipping word: pink -> [79, 676]\n",
      "Skipping word: purple -> [14225, 1154]\n",
      "Skipping word: keyboard -> [2539, 3526]\n",
      "Skipping word: symbol -> [1837, 23650]\n",
      "Skipping word: roof -> [305, 1659]\n"
     ]
    }
   ],
   "source": [
    "def verify_single_tokens(tokenizer, categories: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Verify and filter words to ensure they are single tokens.\"\"\"\n",
    "    if type(categories) == list:\n",
    "        single_token_words = []\n",
    "        for word in categories:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            if len(tokens) == 1:\n",
    "                single_token_words.append(word)\n",
    "            else:\n",
    "                print(f\"Skipping word: {word} -> {tokens}\")\n",
    "        return single_token_words\n",
    "    else:\n",
    "        filtered_categories = {}\n",
    "        for category, words in categories.items():\n",
    "            single_token_words = []\n",
    "            for word in words:\n",
    "                tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(tokens) == 1:\n",
    "                    single_token_words.append(word)\n",
    "                else:\n",
    "                    print(f\"Skipping word: {word} -> {tokens}\")\n",
    "            filtered_categories[category] = single_token_words\n",
    "        return filtered_categories\n",
    "    \n",
    "# Optional: for tokenizer verification (uncomment if using)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "TRAIN_CATEGORIES = verify_single_tokens(tokenizer, TRAIN_CATEGORIES)\n",
    "# OOD_CATEGORIES = verify_single_tokens(tokenizer, OOD_CATEGORIES)\n",
    "DISTRACTORS = verify_single_tokens(tokenizer, DISTRACTORS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "TRAIN_CATEGORIES = verify_single_tokens(tokenizer, TRAIN_CATEGORIES)\n",
    "# OOD_CATEGORIES = verify_single_tokens(tokenizer, OOD_CATEGORIES)\n",
    "DISTRACTORS = verify_single_tokens(tokenizer, DISTRACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb606ce-2eed-41d2-838d-70a181d10625",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train categories:\n",
      "clothing 6\n",
      "animal 7\n",
      "color 7\n",
      "body part 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('distractors: ', 41)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train categories:\")\n",
    "for category, words in TRAIN_CATEGORIES.items():\n",
    "    print(category, len(words))\n",
    "\n",
    "# print(\"\\nood categories:\")\n",
    "# for category, words in OOD_CATEGORIES.items():\n",
    "#     print(category, len(words))\n",
    "\n",
    "\"distractors: \", len(DISTRACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed029ca0-d8bb-4db3-b781-ce81e067dbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clothing': ['shirt', 'pants', 'dress', 'coat', 'vest', 'suit'],\n",
       " 'animal': ['dog', 'cat', 'bird', 'fish', 'wolf', 'fox', 'cow'],\n",
       " 'color': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'brown'],\n",
       " 'body part': ['hand', 'leg', 'ear', 'eye', 'mouth', 'arm', 'foot']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e19c74-0dbc-4269-b59b-788c80274e5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_word_list(category_words: List[str], target_count: int, \n",
    "                      list_length: int, distractors: List[str]) -> List[str]:\n",
    "    \"\"\"Generate a word list with specified target count and total length.\"\"\"\n",
    "    if target_count > len(category_words):\n",
    "        raise ValueError(f\"Target count {target_count} exceeds available words {len(category_words)}\")\n",
    "    if target_count > list_length:\n",
    "        raise ValueError(f\"Target count {target_count} exceeds list length {list_length}\")\n",
    "    \n",
    "    # Select target words\n",
    "    target_words = random.sample(category_words, target_count)\n",
    "    \n",
    "    # Select distractor words\n",
    "    num_distractors = list_length - target_count\n",
    "    distractor_words = random.sample(distractors, num_distractors)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    word_list = target_words + distractor_words\n",
    "    random.shuffle(word_list)\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "def create_example(category: str, category_words: List[str], target_count: int, \n",
    "                  list_length: int, distractors: List[str]) -> Dict:\n",
    "    \"\"\"Create a single training example.\"\"\"\n",
    "    word_list = generate_word_list(category_words, target_count, list_length, distractors)\n",
    "    \n",
    "#     # Create prompt v1\n",
    "#     prompt = f\"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "# Type: {category}\n",
    "# List: [{' '.join(word_list)}]\n",
    "# Answer: (\"\"\"\n",
    "\n",
    "    # # Create prompt v2\n",
    "    # prompt = f\"Analyze the word list below. Count ONLY words that are {category}.\\n\" \\\n",
    "    #          f\"Follow these rules:\\n1. Strictly match the type definition (case-sensitive)\\n\" \\\n",
    "    #          f\"2. Ignore non-word elements\\n3. Output ONLY the integer count inside parentheses\\n\\n\" \\\n",
    "    #          f\"Type: {category}\\nList: [{' '.join(word_list)}]\\nAnswer: \"\n",
    "\n",
    "    # Create prompt v3\n",
    "    prompt = f\"Analyze the word list below. Count ONLY words that are {category}.\\n\" \\\n",
    "             f\"Follow these rules:\\n1. Strictly match the type definition (case-sensitive)\\n\" \\\n",
    "             f\"2. Ignore non-word elements\\n3. Output ONLY one interger count and nothing else\\n\\n\" \\\n",
    "             f\"Type: {category}\\nList: [{' '.join(word_list)}]\\nAnswer: \"\n",
    "\n",
    "    ### Generate corrupted example\n",
    "    \n",
    "    # Identify available replacement words\n",
    "    used_targets = set(word for word in word_list if word in category_words)\n",
    "    used_distractors = set(word for word in word_list if word not in category_words)\n",
    "    unused_targets = [w for w in category_words if w not in used_targets]\n",
    "    unused_distractors = [d for d in distractors if d not in used_distractors]\n",
    "    \n",
    "    # Attempt to corrupt a random word\n",
    "    corrupted_word_list = word_list.copy()\n",
    "    corrupted_target_count = None\n",
    "    \n",
    "    indices = list(range(len(word_list)))\n",
    "    selected_index = random.choice(indices)\n",
    "    \n",
    "    word = corrupted_word_list[selected_index]\n",
    "    if word in category_words:\n",
    "        # if random.randint(0, 1) == 1: \n",
    "        #     # Replace target with distractor (decreases count)\n",
    "        #     new_word = random.choice(unused_distractors)\n",
    "        #     corrupted_word_list[selected_index] = new_word\n",
    "        #     corrupted_target_count = target_count - 1\n",
    "        # else:\n",
    "        #     # Replace target with another target (same count)\n",
    "        #     new_word = random.choice(unused_targets)\n",
    "        #     corrupted_word_list[selected_index] = new_word\n",
    "        #     corrupted_target_count = target_count\n",
    "        # Replace target with distractor (decreases count)\n",
    "        new_word = random.choice(unused_distractors)\n",
    "        corrupted_word_list[selected_index] = new_word\n",
    "        corrupted_target_count = target_count - 1\n",
    "    elif word not in category_words:\n",
    "        # if random.randint(0, 1) == 1: \n",
    "        #     # Replace distractor with target (increases count)\n",
    "        #     new_word = random.choice(unused_targets)\n",
    "        #     corrupted_word_list[selected_index] = new_word\n",
    "        #     corrupted_target_count = target_count + 1\n",
    "        # else:\n",
    "        #     # Replace distractor with another distractor (same count)\n",
    "        #     new_word = random.choice(unused_distractors)\n",
    "        #     corrupted_word_list[selected_index] = new_word\n",
    "        #     corrupted_target_count = target_count\n",
    "        # Replace distractor with target (increases count)\n",
    "        new_word = random.choice(unused_targets)\n",
    "        corrupted_word_list[selected_index] = new_word\n",
    "        corrupted_target_count = target_count + 1\n",
    "    else:\n",
    "        raise RuntimeError(\"No valid replacement found for corruption\")\n",
    "\n",
    "    # Create prompt v3\n",
    "    corrupted_prompt = f\"Analyze the word list below. Count ONLY words that are {category}.\\n\" \\\n",
    "             f\"Follow these rules:\\n1. Strictly match the type definition (case-sensitive)\\n\" \\\n",
    "             f\"2. Ignore non-word elements\\n3. Output ONLY one interger count and nothing else\\n\\n\" \\\n",
    "             f\"Type: {category}\\nList: [{' '.join(corrupted_word_list)}]\\nAnswer: \"\n",
    "    \n",
    "    return {\n",
    "        'category': category,\n",
    "        'list_length': list_length,\n",
    "        'clean_prompt': prompt,\n",
    "        'clean_answer': f\"({target_count})\",\n",
    "        'clean_target_count': target_count,\n",
    "        'clean_word_list': word_list,\n",
    "        'clean_target_positions': [i for i, word in enumerate(word_list) if word in category_words],\n",
    "        'corrupted_position': selected_index,\n",
    "        'corrupted_prompt': corrupted_prompt,\n",
    "        'corrupted_answer': f\"({corrupted_target_count})\",\n",
    "        'corrupted_target_count': corrupted_target_count,\n",
    "        'corrupted_word_list': corrupted_word_list,\n",
    "        'corrupted_target_positions': [i for i, word in enumerate(corrupted_word_list) if word in category_words],\n",
    "    }\n",
    "\n",
    "def generate_dataset_split(categories: Dict[str, List[str]], \n",
    "                          distractors: List[str], \n",
    "                          num_examples: int,\n",
    "                          split_name: str) -> List[Dict]:\n",
    "    \"\"\"Generate a dataset split with balanced examples.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Define distribution parameters\n",
    "    list_lengths = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    list_length_weights = [0.1] * 10\n",
    "    \n",
    "    # Generate examples\n",
    "    for i in range(num_examples):\n",
    "        # Select category\n",
    "        category = random.choice(list(categories.keys()))\n",
    "        category_words = categories[category]\n",
    "        \n",
    "        # Select list length\n",
    "        list_length = random.choices(list_lengths, weights=list_length_weights)[0]\n",
    "        \n",
    "        # Select target count (0 to min(5, list_length))\n",
    "        max_targets = min(5, list_length, len(category_words))\n",
    "        target_count = random.randint(0, max_targets)\n",
    "        \n",
    "        example = create_example(category, category_words, target_count, \n",
    "                               list_length, distractors)\n",
    "        example['split'] = split_name\n",
    "        example['example_id'] = f\"{split_name}_{i:06d}\"\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7ed437-2dec-44db-8f55-f69c2aadad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train categories: ['clothing', 'animal', 'color', 'body part']\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(f\"Train categories: {list(TRAIN_CATEGORIES.keys())}\")\n",
    "# print(f\"OOD test categories: {list(OOD_CATEGORIES.keys())}\")\n",
    "\n",
    "# # Generate train set\n",
    "# train_examples = generate_dataset_split(\n",
    "#     TRAIN_CATEGORIES, DISTRACTORS, \n",
    "#     num_examples=5000, split_name='train'\n",
    "# )\n",
    "\n",
    "# Generate in-distribution test set\n",
    "test_examples = generate_dataset_split(\n",
    "    TRAIN_CATEGORIES, DISTRACTORS, \n",
    "    num_examples=2000, split_name='test'\n",
    ")\n",
    "\n",
    "# # Generate out-of-distribution test set\n",
    "# ood_examples = generate_dataset_split(\n",
    "#     OOD_CATEGORIES, DISTRACTORS, \n",
    "#     num_examples=2000, split_name='ood_test'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be86f5b-d683-4c0c-b5e3-be22eb1ab5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST SET ANALYSIS ===\n",
      "Total examples: 2000\n",
      "Target count distribution:\n",
      "  0: 331 (16.6%)\n",
      "  1: 337 (16.9%)\n",
      "  2: 373 (18.6%)\n",
      "  3: 375 (18.8%)\n",
      "  4: 308 (15.4%)\n",
      "  5: 276 (13.8%)\n",
      "List length distribution:\n",
      "  3: 202 (10.1%)\n",
      "  4: 215 (10.8%)\n",
      "  5: 214 (10.7%)\n",
      "  6: 190 (9.5%)\n",
      "  7: 178 (8.9%)\n",
      "  8: 200 (10.0%)\n",
      "  9: 205 (10.2%)\n",
      "  10: 211 (10.5%)\n",
      "  11: 201 (10.1%)\n",
      "  12: 184 (9.2%)\n",
      "Category distribution:\n",
      "  animal: 488 (24.4%)\n",
      "  body part: 535 (26.8%)\n",
      "  clothing: 498 (24.9%)\n",
      "  color: 479 (23.9%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset(examples: List[Dict], split_name: str):\n",
    "    \"\"\"Print analysis of the generated dataset.\"\"\"\n",
    "    print(f\"\\n=== {split_name.upper()} SET ANALYSIS ===\")\n",
    "    print(f\"Total examples: {len(examples)}\")\n",
    "    \n",
    "    # Count distribution\n",
    "    count_dist = defaultdict(int)\n",
    "    for ex in examples:\n",
    "        count_dist[ex['clean_target_count']] += 1\n",
    "    print(\"Target count distribution:\")\n",
    "    for count in sorted(count_dist.keys()):\n",
    "        print(f\"  {count}: {count_dist[count]} ({count_dist[count]/len(examples)*100:.1f}%)\")\n",
    "    \n",
    "    # Length distribution\n",
    "    length_dist = defaultdict(int)\n",
    "    for ex in examples:\n",
    "        length_dist[ex['list_length']] += 1\n",
    "    print(\"List length distribution:\")\n",
    "    for length in sorted(length_dist.keys()):\n",
    "        print(f\"  {length}: {length_dist[length]} ({length_dist[length]/len(examples)*100:.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    category_dist = defaultdict(int)\n",
    "    for ex in examples:\n",
    "        category_dist[ex['category']] += 1\n",
    "    print(\"Category distribution:\")\n",
    "    for category in sorted(category_dist.keys()):\n",
    "        print(f\"  {category}: {category_dist[category]} ({category_dist[category]/len(examples)*100:.1f}%)\")\n",
    "\n",
    "# Analyze datasets\n",
    "# analyze_dataset(train_examples, 'train')\n",
    "analyze_dataset(test_examples, 'test')\n",
    "# analyze_dataset(ood_examples, 'ood_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "585f703b-ae82-4d9d-82df-5a9a130b793a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "# with open(\"./data/train.json\", 'w') as f:\n",
    "#     json.dump(train_examples, f, indent=2)\n",
    "with open(\"./data/test.json\", 'w') as f:\n",
    "    json.dump(test_examples, f, indent=2)\n",
    "# with open(\"./data/ood_test.json\", 'w') as f:\n",
    "#     json.dump(ood_examples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94168148-5b3a-4dc8-8cb3-0d6c74d79df5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
